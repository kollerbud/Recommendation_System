{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/25 18:26:38 WARN Utils: Your hostname, mainpc resolves to a loopback address: 127.0.1.1; using 172.25.80.73 instead (on interface eth0)\n",
      "24/04/25 18:26:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/25 18:26:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName('Recommendations')\n",
    "                     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "                     .config(\"spark.kryoserializer.buffer.max\", \"2047m\")\n",
    "                     .config(\"spark.driver.memory\", \"15g\")\n",
    "                     .getOrCreate()\n",
    "          )\n",
    "\n",
    "# Set Hadoop configurations to use the service account JSON key\n",
    "# sc = spark.sparkContext\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", f\"../keys/{json_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_business(spark,\n",
    "                           city_name:str ='Philadelphia',\n",
    "                           category:str = 'restaurant',\n",
    "                           min_star_rating: Optional[int]=4,\n",
    "                           min_review_count:int =10):\n",
    "\n",
    "    df = spark.read.csv('./dataset_business.csv', header=True)\n",
    "    df_filter = df.filter(\n",
    "        (func.col('is_open')==1) &\n",
    "        (func.lower(func.col('city'))== city_name.lower()) &\n",
    "        (func.lower(func.col('categories')).contains(category.lower())) &\n",
    "        (func.col('review_count') >= min_review_count)\n",
    "    )\n",
    "    if min_star_rating is not None:\n",
    "        df_filter = df_filter.filter(func.col('stars') >= min_star_rating)\n",
    "\n",
    "    df_select = df_filter.select(\n",
    "        func.col('business_id'),\n",
    "        func.col('categories'),\n",
    "        func.col('name'),\n",
    "        func.col('review_count'),\n",
    "        func.col('stars').alias('business_stars')\n",
    "    )\n",
    "\n",
    "    string_indexer = StringIndexer(inputCol='business_id', outputCol='business_id_encode')\n",
    "    model = string_indexer.fit(df_select)\n",
    "    city_business_num_id = model.transform(df_select)\n",
    "\n",
    "    # Convert encoded business_id to integer type\n",
    "    city_business_num_id = city_business_num_id.withColumn(\n",
    "        'business_id_encode',\n",
    "        func.col('business_id_encode').cast(IntegerType())\n",
    "    )\n",
    "\n",
    "    return city_business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(spark,\n",
    "                      min_review_count: int = 10,\n",
    "                      min_star_rating: Optional[int] = None,\n",
    "                      ) -> DataFrame:\n",
    "\n",
    "    df_reviews = spark.read.json('./yelp_academic_dataset_review.json')\n",
    "\n",
    "    # Group by user_id to calculate review counts per user\n",
    "    df_user_review_counts = df_reviews.groupBy(\"user_id\").agg(\n",
    "        func.count(\"review_id\").alias(\"user_review_count\")\n",
    "    ).filter(func.col(\"user_review_count\") >= min_review_count)\n",
    "\n",
    "    # Join back to the original reviews to filter users by their review counts\n",
    "    df_filtered_reviews = df_reviews.join(\n",
    "        df_user_review_counts, \"user_id\"\n",
    "    )\n",
    "\n",
    "    # Optionally filter by star rating\n",
    "    if min_star_rating is not None:\n",
    "        df_filtered_reviews = df_filtered_reviews.filter(\n",
    "            func.col(\"stars\") >= min_star_rating\n",
    "        )\n",
    "\n",
    "    # Select and rename columns to match the SQL query\n",
    "    df_final = df_filtered_reviews.select(\n",
    "        func.col(\"user_id\"),\n",
    "        func.col(\"business_id\"),\n",
    "        func.col(\"date\"),\n",
    "        func.col(\"review_id\"),\n",
    "        func.col(\"stars\").alias(\"user_stars\"),\n",
    "        func.col(\"text\").alias(\"user_reviews\")\n",
    "    )\n",
    "\n",
    "    # Encode the user_id using StringIndexer\n",
    "    string_indexer = StringIndexer(inputCol='user_id', outputCol='user_id_encode')\n",
    "    model = string_indexer.fit(df_final)\n",
    "    user_reviews_num_id = model.transform(df_final)\n",
    "\n",
    "    # Convert encoded user_id to integer type\n",
    "    user_reviews_num_id = user_reviews_num_id.withColumn(\n",
    "        'user_id_encode',\n",
    "        func.col('user_id_encode').cast(IntegerType())\n",
    "    )\n",
    "\n",
    "    return user_reviews_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = preprocess_review(spark=spark)\n",
    "businesses = preprocessing_business(spark=spark)\n",
    "\n",
    "business_user_review = reviews.join(businesses,\n",
    "                                    on='business_id',\n",
    "                                    how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer, HashingTF, IDF, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    df_reviews: DataFrame,\n",
    "    train_perc: int = 0.7) -> DataFrame:\n",
    "    \n",
    "    #Random shuffle\n",
    "    shuffled_final = df_reviews.orderBy(func.rand(42))\n",
    "\n",
    "    # Define window specification partitioned by the user_id column and corresponding row number\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(\"user_id\")\n",
    "    final_reviews_with_row_number = shuffled_final.withColumn(\"row_number\", func.row_number().over(window_spec))\n",
    "\n",
    "    # Calculate the total number of rows for each user\n",
    "    user_counts = final_reviews_with_row_number.groupBy(\"user_id\").count()\n",
    "\n",
    "    # Calculate the number of rows to include in the training set for each user\n",
    "    user_test_counts = user_counts.withColumn(\"test_count\", (func.col(\"count\") * (1-train_perc)).cast(\"int\"))\n",
    "\n",
    "    #Join row number data with test count\n",
    "    final_with_test_counts = final_reviews_with_row_number.join(user_test_counts, \"user_id\", \"inner\")\n",
    "\n",
    "    # Create training and testing DataFrames for each user\n",
    "    train_data = final_with_test_counts.filter(func.col(\"row_number\") <= (func.col(\"count\") - func.col(\"test_count\")))\n",
    "    test_data = final_with_test_counts.filter(func.col(\"row_number\") > (func.col(\"count\") - func.col(\"test_count\")))\n",
    "\n",
    "    # Drop the intermediate columns\n",
    "    train_data = train_data.drop(\"row_number\", \"count\", \"test_count\")\n",
    "    test_data = test_data.drop(\"row_number\", \"count\", \"test_count\")\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = train_test_split(df_reviews=business_user_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150861, 44853)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial train test datasets size\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ALS model\n",
    "# best rank: 14\n",
    "# best regParam: 0.19\n",
    "als = ALS(userCol='user_id_encode',\n",
    "          itemCol='business_id_encode',\n",
    "          ratingCol='user_stars',\n",
    "          coldStartStrategy='drop',\n",
    "          nonnegative=True,\n",
    "          rank=14,\n",
    "          regParam=0.19\n",
    "          )\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='user_stars',\n",
    "    predictionCol='prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_params = ParamGridBuilder().addGrid(als.rank, [12,13,14]) \\\n",
    "                               .addGrid(als.regParam, [0.17,0.18,0.19]) \\\n",
    "                               .build()        \n",
    "cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=als_params,\n",
    "        evaluator=evaluator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 18:27:47 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:27:49 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:27:51 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:28:16 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:28:30 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:28:39 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:28:41 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:42 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:46 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:47 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:47 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:48 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:48 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:49 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/25 18:28:49 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:49 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:50 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:50 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:50 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:51 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:51 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:51 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:51 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:52 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:52 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:53 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:53 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:53 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:53 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:54 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:54 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:54 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:54 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:55 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:56 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 18:28:58 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:28:58 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 18:30:15 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:30:38 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:30:45 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 18:30:46 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "24/04/25 18:30:54 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0145323467387843\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rec = model.recommendForAllUsers(5)\n",
    "# https://github.com/apache/spark/blob/master/examples/src/main/python/ml/als_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 19:03:07 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "24/04/25 19:03:14 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29167"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_rec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_stars: double (nullable = true)\n",
      " |-- user_reviews: string (nullable = true)\n",
      " |-- user_id_encode: integer (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: string (nullable = true)\n",
      " |-- business_stars: string (nullable = true)\n",
      " |-- business_id_encode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 19:03:29 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n",
      "[Stage 873:===============================================>     (90 + 10) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------------------------------------------------------------------------+\n",
      "|user_id_encode|filtered_recommendations                                                                    |\n",
      "+--------------+--------------------------------------------------------------------------------------------+\n",
      "|34            |[{928, 4.7108455}, {1336, 4.666741}, {1248, 4.455908}, {1378, 4.327839}, {1209, 4.302132}]  |\n",
      "|81            |[{924, 5.427021}, {1336, 5.0612826}, {1044, 5.0513015}, {281, 5.0418916}, {306, 4.99725}]   |\n",
      "|85            |[{924, 5.2225666}, {1336, 5.1744847}, {577, 4.9288197}, {306, 4.8852763}, {928, 4.8720427}] |\n",
      "|101           |[{924, 4.3754816}, {1153, 4.1570244}, {1336, 4.0941257}, {1044, 4.0686407}, {205, 4.057283}]|\n",
      "|211           |[{306, 5.0479345}, {80, 4.727235}, {810, 4.7261477}, {482, 4.695579}, {745, 4.670101}]      |\n",
      "|321           |[{1336, 5.0151424}, {1248, 4.8526}, {928, 4.796123}, {924, 4.772739}, {1209, 4.686487}]     |\n",
      "|322           |[{577, 5.5585704}, {924, 5.189073}, {1336, 5.1863666}, {679, 5.0912}, {1378, 5.0875154}]    |\n",
      "|362           |[{1336, 5.2450695}, {928, 5.1409097}, {924, 5.0372357}, {1209, 4.994282}, {72, 4.919873}]   |\n",
      "|368           |[{1336, 4.7953644}, {1248, 4.5834084}, {928, 4.5822015}, {924, 4.543896}, {1209, 4.514406}] |\n",
      "|375           |[{1336, 5.3610225}, {924, 5.2838793}, {928, 5.11906}, {1245, 5.0393977}, {1171, 5.036692}]  |\n",
      "+--------------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 19:03:43 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_user_rec = user_rec.withColumn(\n",
    "    \"filtered_recommendations\",\n",
    "    func.expr(\"filter(recommendations, item -> item.rating > 4.0)\")\n",
    ")\n",
    "\n",
    "non_empty_user_rec = filtered_user_rec.filter(\n",
    "    func.size(func.col(\"filtered_recommendations\")) > 0\n",
    ")\n",
    "# Display the filtered results\n",
    "non_empty_user_rec.select(\"user_id_encode\", \"filtered_recommendations\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 19:16:37 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 19:17:19 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 19:18:03 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/04/25 19:18:14 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 19:18:17 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/04/25 19:18:19 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "[Stage 1043:=================================================>      (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+---------+----------+------------+--------------+----------+----+------------+--------------+------------------+\n",
      "|user_id|business_id|date|review_id|user_stars|user_reviews|user_id_encode|categories|name|review_count|business_stars|business_id_encode|\n",
      "+-------+-----------+----+---------+----------+------------+--------------+----------+----+------------+--------------+------------------+\n",
      "+-------+-----------+----+---------+----------+------------+--------------+----------+----+------------+--------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# test.filter((func.col('user_id_encode')==928) & (func.col('business_id_encode')==1209)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_review(spark=spark).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_business(spark=spark, min_review_count=3).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text reviews by user\n",
    "def review_text_processing(df_train: DataFrame,\n",
    "                           df_test: DataFrame,\n",
    "                           reivew_col: str = 'user_reviews',\n",
    "                           rating_cut_off: float = 3.0) -> DataFrame:\n",
    "\n",
    "    df_train_lower = df_train.withColumn(reivew_col, func.lower(reivew_col))\n",
    "    df_test_lower = df_test.withColumn(reivew_col, func.lower(reivew_col))\n",
    "    \n",
    "    \n",
    "    # aggregate reviews by business in train\n",
    "    df_train_agg = df_train_lower.groupBy('business_id', 'name').agg(\n",
    "        func.concat_ws(' ', func.collect_list('user_reviews')).alias(reivew_col),\n",
    "        func.mean('business_stars').alias('business_stars'),\n",
    "    ).filter(\n",
    "        func.col('business_stars') >= rating_cut_off\n",
    "    )\n",
    "    # aggregate reviews by user\n",
    "    df_test_agg = df_test_lower.groupBy('user_id').agg(\n",
    "        func.concat_ws(' ', func.collect_list('user_reviews')).alias(reivew_col)\n",
    "    )\n",
    "    # Define the regex tokenizer\n",
    "    regex_tokenizer = RegexTokenizer(\n",
    "        inputCol=reivew_col,\n",
    "        outputCol='words',\n",
    "        pattern=\"\\\\W\"  # This regex splits the text at any non-word character\n",
    "    )\n",
    "    # Define the stopwords remover\n",
    "    stopwords_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "\n",
    "\n",
    "    # tf-idf\n",
    "    hashing_tf = HashingTF(inputCol='filtered_words',\n",
    "                           outputCol='raw_features', numFeatures=100)\n",
    "    idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "    # Define the pipeline with the stages\n",
    "    pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "\n",
    "    # Fit the pipeline to the data and transform the data\n",
    "    model = pipeline.fit(df_train_agg)\n",
    "    train_transformed = model.transform(df_train_agg)\n",
    "    test_transformed = model.transform(df_test_agg)\n",
    "\n",
    "    # remove extra columns\n",
    "    train_transformed = train_transformed.drop(\n",
    "        'date',\n",
    "        'user_id_encode',\n",
    "        'categories',\n",
    "        'review_count',\n",
    "        'business_id_encode',\n",
    "        'words',\n",
    "        'filtered_words',\n",
    "        'raw_features',\n",
    "        reivew_col\n",
    "    )\n",
    "    test_transformed = test_transformed.drop(\n",
    "        'date',\n",
    "        'user_id_encode',\n",
    "        'categories',\n",
    "        'review_count',\n",
    "        'business_id_encode',\n",
    "        'words',\n",
    "        'filtered_words',\n",
    "        'raw_features',\n",
    "        reivew_col\n",
    "    )\n",
    "\n",
    "    for col in train_transformed.columns:\n",
    "        if col != 'features':\n",
    "            train_transformed = train_transformed.withColumnRenamed(col, 'train_'+col)\n",
    "\n",
    "    for col in test_transformed.columns:\n",
    "        if col != 'features':\n",
    "            test_transformed = test_transformed.withColumnRenamed(col, 'test_'+col)\n",
    "\n",
    "    return train_transformed, test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train, df_test = review_text_processing(\n",
    "                        df_train=train,\n",
    "                        df_test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(features1, features2):\n",
    "    return float(float(features1.dot(features2)) / (Vectors.norm(features1, 2) * Vectors.norm(features2, 2)))\n",
    "\n",
    "cosine_similarity_udf = func.udf(cosine_similarity, FloatType())\n",
    "\n",
    "\n",
    "def cosine_recommendation(train_transformed: DataFrame,\n",
    "                          test_transformed: DataFrame,\n",
    "                          sim_cut_off: float = 0.5) -> DataFrame:\n",
    "\n",
    "    normalizer = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "    train_normalized = normalizer.transform(train_transformed).withColumnRenamed(\"norm_features\", \"train_norm_features\")\n",
    "    test_normalized = normalizer.transform(test_transformed).withColumnRenamed(\"norm_features\", \"test_norm_features\")\n",
    "\n",
    "\n",
    "    # Perform a Cartesian join to calculate cosine similarity between every test and train pair\n",
    "    cartesian_df = test_normalized.crossJoin(train_normalized)\n",
    "    \n",
    "    result_df = cartesian_df.withColumn(\n",
    "        'similarity',\n",
    "        cosine_similarity_udf(cartesian_df['test_norm_features'], cartesian_df['train_norm_features'])\n",
    "    )\n",
    "    result_df = result_df.filter(\n",
    "        func.col('similarity') >= sim_cut_off\n",
    "    )\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train, df_test = review_text_processing(\n",
    "                        df_train=train,\n",
    "                        df_test=test,\n",
    "                        rating_cut_off=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14266"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = cosine_recommendation(train_transformed=df_train, test_transformed=df_test, sim_cut_off=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/25 18:40:50 WARN ExtractPythonUDFFromJoinCondition: The join condition:(cosine_similarity(test_norm_features#2547, train_norm_features#2531)#2651 > 0.7) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "[Stage 559:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        test_user_id|          train_name|similarity|\n",
      "+--------------------+--------------------+----------+\n",
      "|-HYCAMf2ml717YD5Y...|IndeBlue Modern I...|0.90363353|\n",
      "|-HYCAMf2ml717YD5Y...|            Spice 28|0.92052335|\n",
      "|-HYCAMf2ml717YD5Y...|     World Cafe Live| 0.8825746|\n",
      "|-HYCAMf2ml717YD5Y...|    Colombian Bakery|0.78020746|\n",
      "|-HYCAMf2ml717YD5Y...|   Dawson Street Pub| 0.8617899|\n",
      "|-HYCAMf2ml717YD5Y...|Linda's Vegetaria...|0.80385995|\n",
      "|-HYCAMf2ml717YD5Y...|     Zorba's Taverna|  0.882695|\n",
      "|-HYCAMf2ml717YD5Y...|              Martha| 0.8830395|\n",
      "|-HYCAMf2ml717YD5Y...|      Federal Donuts| 0.7904856|\n",
      "|-HYCAMf2ml717YD5Y...|Hue Fusion Food M...| 0.7915747|\n",
      "|-HYCAMf2ml717YD5Y...|M2O Burgers & Salads| 0.8582055|\n",
      "|-HYCAMf2ml717YD5Y...|Nori Ramen & Poke...| 0.7444716|\n",
      "|-HYCAMf2ml717YD5Y...|Ambrosia Ristoran...| 0.8726993|\n",
      "|-HYCAMf2ml717YD5Y...|Prima Pizza Taque...|0.85604244|\n",
      "|-HYCAMf2ml717YD5Y...| Gran Caffe L'Aquila|0.89863795|\n",
      "|-HYCAMf2ml717YD5Y...|Sonny's Famous St...| 0.8821291|\n",
      "|-HYCAMf2ml717YD5Y...|      DaMò Pasta Lab| 0.8638432|\n",
      "|-HYCAMf2ml717YD5Y...|         Martabak Ok| 0.8421584|\n",
      "|-HYCAMf2ml717YD5Y...|            Le Virtu|0.91510016|\n",
      "|-HYCAMf2ml717YD5Y...|      Pizzeria Vetri| 0.8037586|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.select('test_user_id', 'train_name', 'similarity').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6740_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
