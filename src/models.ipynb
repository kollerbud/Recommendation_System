{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/04/27 02:02:49 WARN Utils: Your hostname, mainpc resolves to a loopback address: 127.0.1.1; using 172.25.80.73 instead (on interface eth0)\n",
      "24/04/27 02:02:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/27 02:02:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "                     .appName('Recommendations')\n",
    "                     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "                     .config(\"spark.kryoserializer.buffer.max\", \"2047m\")\n",
    "                     .config(\"spark.driver.memory\", \"15g\")\n",
    "                     .getOrCreate()\n",
    "          )\n",
    "\n",
    "# Set Hadoop configurations to use the service account JSON key\n",
    "# sc = spark.sparkContext\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", f\"../keys/{json_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_business(spark,\n",
    "                           city_name:str ='Philadelphia',\n",
    "                           category:str = 'restaurant',\n",
    "                           min_star_rating: Optional[int]= None,\n",
    "                           min_review_count:int =10):\n",
    "\n",
    "    df = spark.read.csv('./dataset_business.csv', header=True)\n",
    "    df_filter = df.filter(\n",
    "        (func.col('is_open')==1) &\n",
    "        (func.lower(func.col('city'))== city_name.lower()) &\n",
    "        (func.lower(func.col('categories')).contains(category.lower())) &\n",
    "        (func.col('review_count') >= min_review_count)\n",
    "    )\n",
    "    if min_star_rating is not None:\n",
    "        df_filter = df_filter.filter(func.col('stars') >= min_star_rating)\n",
    "\n",
    "    df_select = df_filter.select(\n",
    "        func.col('business_id'),\n",
    "        func.col('categories'),\n",
    "        func.col('name'),\n",
    "        func.col('review_count'),\n",
    "        func.col('stars').alias('business_stars')\n",
    "    )\n",
    "\n",
    "    string_indexer = StringIndexer(inputCol='business_id', outputCol='business_id_encode')\n",
    "    model = string_indexer.fit(df_select)\n",
    "    city_business_num_id = model.transform(df_select)\n",
    "\n",
    "    # Convert encoded business_id to integer type\n",
    "    city_business_num_id = city_business_num_id.withColumn(\n",
    "        'business_id_encode',\n",
    "        func.col('business_id_encode').cast(IntegerType())\n",
    "    )\n",
    "\n",
    "    return city_business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(spark,\n",
    "                      min_review_count: int = 10,\n",
    "                      min_star_rating: Optional[int] = None,\n",
    "                      ) -> DataFrame:\n",
    "\n",
    "    df_reviews = spark.read.json('./slim_review.json')\n",
    "\n",
    "    # Group by user_id to calculate review counts per user\n",
    "    df_user_review_counts = df_reviews.groupBy(\"user_id\").agg(\n",
    "        func.count(\"review_id\").alias(\"user_review_count\")\n",
    "    ).filter(func.col(\"user_review_count\") >= min_review_count)\n",
    "\n",
    "    # Join back to the original reviews to filter users by their review counts\n",
    "    df_filtered_reviews = df_reviews.join(\n",
    "        df_user_review_counts, \"user_id\"\n",
    "    )\n",
    "\n",
    "    # Optionally filter by star rating\n",
    "    if min_star_rating is not None:\n",
    "        df_filtered_reviews = df_filtered_reviews.filter(\n",
    "            func.col(\"stars\") >= min_star_rating\n",
    "        )\n",
    "\n",
    "    # Select and rename columns to match the SQL query\n",
    "    df_final = df_filtered_reviews.select(\n",
    "        func.col(\"user_id\"),\n",
    "        func.col(\"business_id\"),\n",
    "        func.col(\"date\"),\n",
    "        func.col(\"review_id\"),\n",
    "        func.col(\"stars\").alias(\"user_stars\"),\n",
    "        func.col(\"text\").alias(\"user_reviews\")\n",
    "    )\n",
    "\n",
    "    # Encode the user_id using StringIndexer\n",
    "    string_indexer = StringIndexer(inputCol='user_id', outputCol='user_id_encode')\n",
    "    model = string_indexer.fit(df_final)\n",
    "    user_reviews_num_id = model.transform(df_final)\n",
    "\n",
    "    # Convert encoded user_id to integer type\n",
    "    user_reviews_num_id = user_reviews_num_id.withColumn(\n",
    "        'user_id_encode',\n",
    "        func.col('user_id_encode').cast(IntegerType())\n",
    "    )\n",
    "\n",
    "    return user_reviews_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = preprocess_review(spark=spark)\n",
    "businesses = preprocessing_business(spark=spark)\n",
    "\n",
    "business_user_review = reviews.join(businesses,\n",
    "                                    on='business_id',\n",
    "                                    how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: string (nullable = true)\n",
      " |-- business_stars: string (nullable = true)\n",
      " |-- business_id_encode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "businesses.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_stars: long (nullable = true)\n",
      " |-- user_reviews: string (nullable = true)\n",
      " |-- user_id_encode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+--------------------+----------+--------------------+--------------+\n",
      "|             user_id|         business_id|               date|           review_id|user_stars|        user_reviews|user_id_encode|\n",
      "+--------------------+--------------------+-------------------+--------------------+----------+--------------------+--------------+\n",
      "|-Cw2rJx6v8gHgWOBX...|6_T2xzR74JqGCTPef...|2011-08-16 00:08:57|bZBN5lFvhz1UebCmj...|         5|FRESH FRESH FRESH...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|PP3BBaVxZLcJU54uP...|2011-08-29 15:15:25|iZGzgccv0AXwEidKw...|         4|How could you not...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|hX_8ZUmIqWFWzjdiP...|2012-01-20 19:50:08|WlxKsYqEtaGYnAdoT...|         4|Ordered the pigno...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|ntiIq1FNqduOyyowM...|2011-08-16 00:10:12|rYV5irskfexITtv-p...|         5|affordable and ve...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|IWHdx0NhDKADkGOgX...|2011-08-16 01:03:23|MveqEVKQQYy-V70x9...|         3|very unique. the ...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|ytynqOUb3hjKeJfRj...|2012-01-20 19:45:59|Gi8AcZSfQ3kEyEXnv...|         4|Came back here fo...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|3ldp8B52AOzNSFnPM...|2013-08-13 16:42:51|4UXVL7bCD5mrqLjxK...|         4|I NEVER eat Beef ...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|6_T2xzR74JqGCTPef...|2012-09-04 21:05:10|XHhJnYrxpUC0nWC9j...|         5|Went here again w...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|838bEDzZSPveDkAGr...|2012-09-04 21:11:58|RUIW-C4LuF0L9N5T-...|         4|Packed, I'm givin...|          5444|\n",
      "|-Cw2rJx6v8gHgWOBX...|cXSyVvOr9YRN9diDk...|2011-08-16 00:13:29|zKullxpbsq4VRNzg2...|         3|WOW! We waited 1....|          5444|\n",
      "+--------------------+--------------------+-------------------+--------------------+----------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_stars: long (nullable = true)\n",
      " |-- user_reviews: string (nullable = true)\n",
      " |-- user_id_encode: integer (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- review_count: string (nullable = true)\n",
      " |-- business_stars: string (nullable = true)\n",
      " |-- business_id_encode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "business_user_review.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import StopWordsRemover, RegexTokenizer, HashingTF, IDF, Normalizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    df_reviews: DataFrame,\n",
    "    train_perc: int = 0.7) -> DataFrame:\n",
    "    \n",
    "    #Random shuffle\n",
    "    shuffled_final = df_reviews.orderBy(func.rand(42))\n",
    "\n",
    "    # Define window specification partitioned by the user_id column and corresponding row number\n",
    "    window_spec = Window.partitionBy(\"user_id\").orderBy(\"user_id\")\n",
    "    final_reviews_with_row_number = shuffled_final.withColumn(\"row_number\", func.row_number().over(window_spec))\n",
    "\n",
    "    # Calculate the total number of rows for each user\n",
    "    user_counts = final_reviews_with_row_number.groupBy(\"user_id\").count()\n",
    "\n",
    "    # Calculate the number of rows to include in the training set for each user\n",
    "    user_test_counts = user_counts.withColumn(\"test_count\", (func.col(\"count\") * (1-train_perc)).cast(\"int\"))\n",
    "\n",
    "    #Join row number data with test count\n",
    "    final_with_test_counts = final_reviews_with_row_number.join(user_test_counts, \"user_id\", \"inner\")\n",
    "\n",
    "    # Create training and testing DataFrames for each user\n",
    "    train_data = final_with_test_counts.filter(func.col(\"row_number\") <= (func.col(\"count\") - func.col(\"test_count\")))\n",
    "    test_data = final_with_test_counts.filter(func.col(\"row_number\") > (func.col(\"count\") - func.col(\"test_count\")))\n",
    "\n",
    "    # Drop the intermediate columns\n",
    "    train_data = train_data.drop(\"row_number\", \"count\", \"test_count\")\n",
    "    test_data = test_data.drop(\"row_number\", \"count\", \"test_count\")\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = train_test_split(df_reviews=business_user_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(142445, 56510)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial train test datasets size\n",
    "train.count(), test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ALS model\n",
    "# best rank: 14\n",
    "# best regParam: 0.19\n",
    "als = ALS(userCol='user_id_encode',\n",
    "          itemCol='business_id_encode',\n",
    "          ratingCol='user_stars',\n",
    "          coldStartStrategy='drop',\n",
    "          nonnegative=True,\n",
    "          rank=14,\n",
    "          regParam=0.19\n",
    "          )\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='user_stars',\n",
    "    predictionCol='prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_params = ParamGridBuilder().addGrid(als.rank, [12,13,14]) \\\n",
    "                               .addGrid(als.regParam, [0.17,0.18,0.19]) \\\n",
    "                               .build()        \n",
    "cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=als_params,\n",
    "        evaluator=evaluator\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 02:03:19 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0269023512011684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rec = model.recommendForAllUsers(5)\n",
    "# https://github.com/apache/spark/blob/master/examples/src/main/python/ml/als_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_rec.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_user_rec = user_rec.withColumn(\n",
    "#     \"filtered_recommendations\",\n",
    "#     func.expr(\"filter(recommendations, item -> item.rating > 4.0)\")\n",
    "# )\n",
    "\n",
    "# non_empty_user_rec = filtered_user_rec.filter(\n",
    "#     func.size(func.col(\"filtered_recommendations\")) > 0\n",
    "# )\n",
    "# # Display the filtered results\n",
    "# non_empty_user_rec.select(\"user_id_encode\", \"filtered_recommendations\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.filter((func.col('user_id_encode')==928) & (func.col('business_id_encode')==1209)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_review(spark=spark).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_business(spark=spark, min_review_count=3).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text reviews by user\n",
    "def review_text_processing(df_train: DataFrame,\n",
    "                           df_test: DataFrame,\n",
    "                           reivew_col: str = 'user_reviews',\n",
    "                           rating_cut_off: float = 0) -> DataFrame:\n",
    "\n",
    "    df_train_lower = df_train.withColumn(reivew_col, func.lower(reivew_col))\n",
    "    df_test_lower = df_test.withColumn(reivew_col, func.lower(reivew_col))\n",
    "    \n",
    "    \n",
    "    # aggregate reviews by business in train\n",
    "    df_train_agg = df_train_lower.groupBy('business_id', 'name').agg(\n",
    "        func.concat_ws(' ', func.collect_list('user_reviews')).alias(reivew_col),\n",
    "        func.mean('business_stars').alias('business_stars'),\n",
    "    ).filter(\n",
    "        func.col('business_stars') >= rating_cut_off\n",
    "    )\n",
    "    # aggregate reviews by user\n",
    "    df_test_agg = df_test_lower.groupBy('user_id').agg(\n",
    "        func.concat_ws(' ', func.collect_list('user_reviews')).alias(reivew_col)\n",
    "    )\n",
    "    # Define the regex tokenizer\n",
    "    regex_tokenizer = RegexTokenizer(\n",
    "        inputCol=reivew_col,\n",
    "        outputCol='words',\n",
    "        pattern=\"\\\\W\"  # This regex splits the text at any non-word character\n",
    "    )\n",
    "    # Define the stopwords remover\n",
    "    stopwords_remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "\n",
    "    # tf-idf\n",
    "    hashing_tf = HashingTF(inputCol='filtered_words',\n",
    "                           outputCol='raw_features', numFeatures=100)\n",
    "    idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "    # Define the pipeline with the stages\n",
    "    pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "\n",
    "    # Fit the pipeline to the data and transform the data\n",
    "    model = pipeline.fit(df_train_agg)\n",
    "    train_transformed = model.transform(df_train_agg)\n",
    "    test_transformed = model.transform(df_test_agg)\n",
    "\n",
    "    # remove extra columns\n",
    "    train_transformed = train_transformed.drop(\n",
    "        'date',\n",
    "        # 'user_id',\n",
    "        'categories',\n",
    "        'review_count',\n",
    "        # 'business_id',\n",
    "        'words',\n",
    "        'filtered_words',\n",
    "        'raw_features',\n",
    "        reivew_col\n",
    "    )\n",
    "    test_transformed = test_transformed.drop(\n",
    "        'date',\n",
    "        # 'user_id',\n",
    "        'categories',\n",
    "        'review_count',\n",
    "        # 'business_id',\n",
    "        'words',\n",
    "        'filtered_words',\n",
    "        'raw_features',\n",
    "        reivew_col\n",
    "    )\n",
    "\n",
    "    for col in train_transformed.columns:\n",
    "        if col != 'features':\n",
    "            train_transformed = train_transformed.withColumnRenamed(col, 'train_'+col)\n",
    "\n",
    "    for col in test_transformed.columns:\n",
    "        if col != 'features':\n",
    "            test_transformed = test_transformed.withColumnRenamed(col, 'test_'+col)\n",
    "\n",
    "    return train_transformed, test_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(features1, features2):\n",
    "    return float(float(features1.dot(features2)) / (Vectors.norm(features1, 2) * Vectors.norm(features2, 2)))\n",
    "\n",
    "cosine_similarity_udf = func.udf(cosine_similarity, FloatType())\n",
    "\n",
    "def cosine_recommendation(train_transformed: DataFrame,\n",
    "                          test_transformed: DataFrame,\n",
    "                          sim_cut_off: float = 0.5) -> DataFrame:\n",
    "\n",
    "    normalizer = Normalizer(inputCol='features', outputCol='norm_features', p=2.0)\n",
    "    train_normalized = normalizer.transform(train_transformed).withColumnRenamed(\"norm_features\", \"train_norm_features\")\n",
    "    test_normalized = normalizer.transform(test_transformed).withColumnRenamed(\"norm_features\", \"test_norm_features\")\n",
    "\n",
    "\n",
    "    # Perform a Cartesian join to calculate cosine similarity between every test and train pair\n",
    "    cartesian_df = test_normalized.crossJoin(train_normalized)\n",
    "    \n",
    "    result_df = cartesian_df.withColumn(\n",
    "        'similarity',\n",
    "        cosine_similarity_udf(cartesian_df['test_norm_features'], cartesian_df['train_norm_features'])\n",
    "    )\n",
    "    result_df = result_df.filter(\n",
    "        func.col('similarity') >= sim_cut_off\n",
    "    )\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hit_rate(ground_truth_df, # test data\n",
    "                 recommended_df, # result_df\n",
    "                 user_col, # user_id\n",
    "                 item_col # business_id\n",
    "                 ):\n",
    "    # Join ground truth and recommended items on user ID\n",
    "    joined_df = ground_truth_df.join(recommended_df, on=user_col)\n",
    "\n",
    "    # Filter to count hits (correct recommendations) for each user\n",
    "    hits_df = joined_df.filter(func.col(\"recommended_\" + item_col) == func.col(item_col)) \\\n",
    "                       .select(user_col).distinct()\n",
    "\n",
    "    # Count the distinct users with at least one correct recommendation\n",
    "    users_with_hits = hits_df.select(func.countDistinct(user_col)).collect()[0][0]\n",
    "\n",
    "    # Count the total number of distinct users for whom recommendations were made\n",
    "    total_users = recommended_df.select(func.countDistinct(user_col)).collect()[0][0]\n",
    "\n",
    "    # Calculate total hit rate\n",
    "    total_hit_rate = users_with_hits / total_users\n",
    "\n",
    "    return total_hit_rate\n",
    "\n",
    "def get_recall(ground_truth_df,\n",
    "               recommended_df,\n",
    "               user_col,\n",
    "               item_col):\n",
    "    # Join ground truth and recommended items on user ID\n",
    "    joined_df = ground_truth_df.join(recommended_df, on=user_col)\n",
    "\n",
    "    # Count the number of relevant recommended items and total relevant items for each user\n",
    "    relevant_recommended_items = joined_df.filter(func.col(\"recommended_\"+item_col) == func.col(item_col))\n",
    "    total_relevant_items = ground_truth_df.groupBy(user_col).count()\n",
    "\n",
    "    # Compute recall for each user\n",
    "    recall_df = relevant_recommended_items.groupBy(user_col).count().withColumnRenamed(\"count\", \"relevant_count\") \\\n",
    "        .join(total_relevant_items, on=user_col)\n",
    "    recall_df = recall_df.withColumn(\"recall\", func.col(\"relevant_count\") / func.col(\"count\"))\n",
    "\n",
    "    # Average recall across all users\n",
    "    average_recall = recall_df.selectExpr(\"avg(recall)\").collect()[0][0]\n",
    "\n",
    "    return average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train, df_test = review_text_processing(\n",
    "                        df_train=train,\n",
    "                        df_test=test,\n",
    "                        rating_cut_off=4)  # only 4-star or above restaurant will be recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7725"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = cosine_recommendation(train_transformed=df_train, test_transformed=df_test, sim_cut_off=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- test_user_id: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- test_norm_features: vector (nullable = true)\n",
      " |-- train_business_id: string (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- train_business_stars: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- train_norm_features: vector (nullable = true)\n",
      " |-- similarity: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_df.select(\"test_user_id_encode\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 02:03:35 WARN ExtractPythonUDFFromJoinCondition: The join condition:(cosine_similarity(test_norm_features#1222, train_norm_features#1206)#1314 > 0.7) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "[Stage 357:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|        test_user_id|   train_business_id|similarity|\n",
      "+--------------------+--------------------+----------+\n",
      "|-Cw2rJx6v8gHgWOBX...|1vPSY4EA-fTRIZYz1...|0.73268765|\n",
      "|-Cw2rJx6v8gHgWOBX...|2gFPQCmKKVi1aHgGT...| 0.7082998|\n",
      "|-Cw2rJx6v8gHgWOBX...|4zQV6v8TwEYMwI9Ek...|0.71370053|\n",
      "|-Cw2rJx6v8gHgWOBX...|5R3-eCIk4dRBtXo0A...| 0.7614566|\n",
      "|-Cw2rJx6v8gHgWOBX...|9Q_dbzylYiWdF11lH...|0.70663893|\n",
      "|-Cw2rJx6v8gHgWOBX...|CpBuc0aSueBpTXce3...|0.71562934|\n",
      "|-Cw2rJx6v8gHgWOBX...|EsQZIf_5Typ-plWtO...|0.71205634|\n",
      "|-Cw2rJx6v8gHgWOBX...|FIFi_8eNmc-jPHZVH...| 0.7016219|\n",
      "|-Cw2rJx6v8gHgWOBX...|HFuAKE0uZ-frIMjro...|0.70307285|\n",
      "|-Cw2rJx6v8gHgWOBX...|JVDHxMnKjif8XdXVF...|0.70583504|\n",
      "|-Cw2rJx6v8gHgWOBX...|QJOC6Uz-RCpzPB6aM...|0.71362644|\n",
      "|-Cw2rJx6v8gHgWOBX...|Uky0DD3LU4C7eyNDh...|0.77991164|\n",
      "|-Cw2rJx6v8gHgWOBX...|VRnJgj0IvxoksHppE...|0.70383286|\n",
      "|-Cw2rJx6v8gHgWOBX...|Wfg85U89qt1OA7aqw...|0.72462535|\n",
      "|-Cw2rJx6v8gHgWOBX...|Yu_QofgDAjn__QsMi...|0.72924453|\n",
      "|-Cw2rJx6v8gHgWOBX...|_gaOrkYtODrxY0JlW...| 0.7059694|\n",
      "|-Cw2rJx6v8gHgWOBX...|arKiXax3ScSM_z3O-...| 0.7639595|\n",
      "|-Cw2rJx6v8gHgWOBX...|cM77NrFZx2z0d84Aq...| 0.7286697|\n",
      "|-Cw2rJx6v8gHgWOBX...|ceSjm0nHamTmAd1tW...| 0.7069498|\n",
      "|-Cw2rJx6v8gHgWOBX...|dChRGpit9fM_kZK5p...| 0.7158909|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.select('test_user_id', 'train_business_id', 'similarity').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.507991027618113"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7247/14266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- test_user_id: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- test_norm_features: vector (nullable = true)\n",
      " |-- train_business_id: string (nullable = true)\n",
      " |-- train_name: string (nullable = true)\n",
      " |-- train_business_stars: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- train_norm_features: vector (nullable = true)\n",
      " |-- similarity: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/27 02:05:19 WARN ExtractPythonUDFFromJoinCondition: The join condition:(cosine_similarity(test_norm_features#1222, train_norm_features#1206)#1314 > 0.7) of the join plan contains PythonUDF only, it will be moved out and the join plan will be turned to cross join.\n",
      "[Stage 439:====>                                                  (1 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5453596746914247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# recall calculation\n",
    "\n",
    "# find distinct user-restaurant pair (total restaurants user has visit)\n",
    "total_visits_pair = test.select('user_id', 'business_id').distinct()\n",
    "# recommend user-restaurant pair\n",
    "rec_visits_pair = result_df.select('test_user_id', 'train_business_id').distinct()\n",
    "\n",
    "# recommend user-restaurant pair in test user-restaurant pair\n",
    "actual_vist = total_visits_pair.join(\n",
    "                rec_visits_pair,\n",
    "                (total_visits_pair['user_id'] == rec_visits_pair['test_user_id']) &\n",
    "                (total_visits_pair['business_id']==rec_visits_pair['train_business_id']),\n",
    "                how='inner'\n",
    "            )\n",
    "\n",
    "total_visits = total_visits_pair.count()\n",
    "recall = actual_vist.count() / total_visits\n",
    "\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6740_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
