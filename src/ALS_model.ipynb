{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: credentialsFile\n",
      "Warning: Ignoring non-Spark config property: parentProject\n",
      "24/03/23 07:37:36 WARN Utils: Your hostname, codespaces-b70f2a resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/23 07:37:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/23 07:37:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "json_key = os.getenv('auth_json')\n",
    "spark = (SparkSession.builder\n",
    "                     .appName('Recommendations')\n",
    "                     .config('spark.jars', 'https://storage.googleapis.com/spark-lib/bigquery/spark-3.4-bigquery-0.34.0.jar')\n",
    "                     .config('credentialsFile', f\"../keys/{json_key}\")\n",
    "                     .config('parentProject', f'{os.getenv(\"project\")}')\n",
    "                     .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "                     .config(\"spark.kryoserializer.buffer.max\", \"2047m\")\n",
    "                     .getOrCreate()\n",
    "        )\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "# Set Hadoop configurations to use the service account JSON key\n",
    "# sc = spark.sparkContext\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "# sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", f\"../keys/{json_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files in bucket\n",
    "# gs://yelp_data_6740/yelp_data/yelp_academic_dataset_business.json\n",
    "# gs://yelp_data_6740/yelp_data/yelp_academic_dataset_checkin.json\n",
    "# gs://yelp_data_6740/yelp_data/yelp_academic_dataset_review.json\n",
    "# gs://yelp_data_6740/yelp_data/yelp_academic_dataset_tip.json\n",
    "# gs://yelp_data_6740/yelp_data/yelp_academic_dataset_user.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(bucket_path):\n",
    "    return (spark.read.json(bucket_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_business(spark,\n",
    "                           city_name:str ='Philadelphia',\n",
    "                           category:str = 'restaurant',\n",
    "                           min_review_count:int =10):\n",
    "    spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "    spark.conf.set(\"materializationDataset\",\"yelp_data\")\n",
    "\n",
    "\n",
    "    sql_statement = f'''\n",
    "        SELECT business_id, categories, name, review_count, stars AS business_stars\n",
    "        FROM `{os.getenv(\"project\")}.{os.getenv(\"dataset\")}.business`\n",
    "        WHERE is_open = 1\n",
    "        AND LOWER(city) = '{city_name.lower()}'\n",
    "        AND LOWER(categories) LIKE '%{category.lower()}%'\n",
    "        AND review_count >= {min_review_count}\n",
    "        ;\n",
    "    '''\n",
    "    # select columns to be used, rename to avoid name collision\n",
    "    city_business = (\n",
    "        spark.read\n",
    "             .format('bigquery')\n",
    "             .option('query', sql_statement)\n",
    "             .option(\"materializationExpirationTimeInMinutes\", 10)\n",
    "             .load()\n",
    "    )\n",
    "    \n",
    "    string_indexer = StringIndexer(inputCol='business_id', outputCol='business_id_encode')\n",
    "    model = string_indexer.fit(city_business)\n",
    "    \n",
    "    city_business_num_id = model.transform(city_business) \\\n",
    "                                .withColumn(\n",
    "                                    'business_id_encode',\n",
    "                                    func.col('business_id_encode').cast(IntegerType())\n",
    "                                )\n",
    "\n",
    "    return city_business_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing_business(spark=spark, category='restaurant').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(spark,\n",
    "                      min_review_count:int = 10,\n",
    "                      cutoff_date=None,\n",
    "                      ):\n",
    "    # https://github.com/GoogleCloudDataproc/spark-bigquery-connector/tree/master\n",
    "    spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "    spark.conf.set(\"materializationDataset\",\"yelp_data\")\n",
    "    \n",
    "    sql_statement = f'''\n",
    "        SELECT r.user_id, r.business_id, r.date, r.review_id, r.stars AS user_stars\n",
    "        FROM `{os.getenv(\"project\")}.{os.getenv(\"dataset\")}.reviews` r\n",
    "        INNER JOIN (\n",
    "            SELECT user_id, COUNT(review_id) AS user_review_count\n",
    "            FROM `{os.getenv(\"project\")}.{os.getenv(\"dataset\")}.reviews`\n",
    "            GROUP BY user_id\n",
    "            HAVING user_review_count >= {min_review_count}\n",
    "        ) rc\n",
    "        ON r.user_id = rc.user_id\n",
    "        ;\n",
    "    ''' \n",
    "    user_reviews = (\n",
    "        spark.read\n",
    "             .format('bigquery')\n",
    "             .option('query', sql_statement)\n",
    "             .option(\"materializationExpirationTimeInMinutes\", 10)\n",
    "             .load()\n",
    "    )\n",
    "    string_indexer = StringIndexer(inputCol='user_id', outputCol='user_id_encode')\n",
    "    model = string_indexer.fit(user_reviews)\n",
    "\n",
    "    user_reviews_num_id = model.transform(user_reviews) \\\n",
    "                               .withColumn(\n",
    "                                   'user_id_encode',\n",
    "                                   func.col('user_id_encode').cast(IntegerType())\n",
    "                               )\n",
    "    return user_reviews_num_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_review(spark=spark).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 07:37:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reviews = preprocess_review(spark=spark)\n",
    "businesses = preprocessing_business(spark=spark)\n",
    "\n",
    "business_user_review = reviews.join(businesses,\n",
    "                                    on='business_id',\n",
    "                                    how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = business_user_review.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# make ALS model\n",
    "als = ALS(userCol='user_id_encode',\n",
    "          itemCol='business_id_encode',\n",
    "          ratingCol='user_stars',\n",
    "          coldStartStrategy='drop',\n",
    "          nonnegative=True,\n",
    "          rank=14,\n",
    "          regParam=0.19\n",
    "          )\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='user_stars',\n",
    "    predictionCol='prediction'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_params = ParamGridBuilder().addGrid(als.rank, [12,13,14]) \\\n",
    "                               .addGrid(als.regParam, [0.17,0.18,0.19]) \\\n",
    "                               .build()        \n",
    "cv = CrossValidator(\n",
    "        estimator=als,\n",
    "        estimatorParamMaps=als_params,\n",
    "        evaluator=evaluator\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cv.fit(train)\n",
    "# best rank: 14\n",
    "# best regParam: 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 07:38:01 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "24/03/23 07:38:06 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "24/03/23 07:38:10 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:12 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:14 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:15 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:16 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:17 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:18 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/03/23 07:38:18 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/03/23 07:38:18 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:19 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:20 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:21 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:21 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:22 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:22 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:23 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:23 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:24 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:25 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:25 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:26 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:26 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:27 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:27 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:27 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:28 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:29 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:29 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:29 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 07:38:31 WARN DAGScheduler: Broadcasting large task binary with size 5.6 MiB\n",
      "24/03/23 07:38:31 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:31 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:35 WARN DAGScheduler: Broadcasting large task binary with size 5.7 MiB\n",
      "24/03/23 07:38:38 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:38 WARN DAGScheduler: Broadcasting large task binary with size 5.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.135459922938278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_rec = model.recommendForAllUsers(2)\n",
    "# https://github.com/apache/spark/blob/master/examples/src/main/python/ml/als_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 07:38:39 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "24/03/23 07:38:47 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|user_id_encode|     recommendations|\n",
      "+--------------+--------------------+\n",
      "|            12|[{1873, 5.177342}...|\n",
      "|            13|[{2464, 5.362191}...|\n",
      "|            22|[{1670, 6.202652}...|\n",
      "|            26|[{1873, 4.588222}...|\n",
      "|            34|[{1873, 4.77135},...|\n",
      "|            44|[{1873, 5.8244104...|\n",
      "|            52|[{829, 4.8981504}...|\n",
      "|            65|[{1823, 1.1047455...|\n",
      "|            81|[{1873, 5.3039203...|\n",
      "|            91|[{1670, 4.8955626...|\n",
      "|           101|[{2005, 5.2678757...|\n",
      "|           103|[{1873, 5.266436}...|\n",
      "|           132|[{1873, 4.647034}...|\n",
      "|           140|[{1823, 4.6282005...|\n",
      "|           146|[{1433, 5.57479},...|\n",
      "|           148|[{611, 4.0818186}...|\n",
      "|           190|[{1823, 4.743745}...|\n",
      "|           192|[{2717, 4.6779294...|\n",
      "|           209|[{1873, 4.4294596...|\n",
      "|           211|[{2005, 4.975859}...|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_rec.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "6740_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
