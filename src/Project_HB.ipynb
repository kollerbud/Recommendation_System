{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRjV_Bdai1s4",
        "outputId": "37cd1751-bb7e-4624-f7fb-461374be4abe"
      },
      "id": "eRjV_Bdai1s4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=a1cf943c1c6adf618a3754c652baf31ff8a8eadeb15e7e25720694e9645eaade\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15302750",
      "metadata": {
        "id": "15302750"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType, BooleanType, MapType, FloatType, ArrayType, DoubleType\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "from pyspark.sql.functions import rand\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from google.colab import files\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e45ff709",
      "metadata": {
        "id": "e45ff709"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Restaurant Recommendation\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.conf.set(\"spark.sql.legacy.style\", \"false\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrf2WeWKjU7U",
        "outputId": "d6028bf7-9f47-4a06-fefe-f6cf6dbced35"
      },
      "id": "jrf2WeWKjU7U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType([\n",
        "    StructField(\"business_id\", StringType()),\n",
        "    StructField(\"name\", StringType()),\n",
        "    StructField(\"address\", StringType()),\n",
        "    StructField(\"city\", StringType()),\n",
        "    StructField(\"state\", StringType()),\n",
        "    StructField(\"postal_code\", StringType()),\n",
        "    StructField(\"latitude\", FloatType()),\n",
        "    StructField(\"longitude\", FloatType()),\n",
        "    StructField(\"stars\", FloatType()),\n",
        "    StructField(\"review_count\", IntegerType()),\n",
        "    StructField(\"is_open\", IntegerType()),\n",
        "    StructField(\"attributes\", MapType(StringType(), StringType())),\n",
        "    StructField(\"categories\", StringType()),\n",
        "    StructField(\"hours\", MapType(StringType(), StringType()))\n",
        "])"
      ],
      "metadata": {
        "id": "BCVxrS1pt5gr"
      },
      "id": "BCVxrS1pt5gr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04fe399b",
      "metadata": {
        "id": "04fe399b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "519f3501-a54c-48e6-eb71-0e50ca16405f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/drive/Shared with me/ISyE 6740 - Computational Analytics/Project/yelp_academic_dataset_business.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f7666273c866>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbusiness_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/Shared with me/ISyE 6740 - Computational Analytics/Project/yelp_academic_dataset_business.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/drive/Shared with me/ISyE 6740 - Computational Analytics/Project/yelp_academic_dataset_business.json."
          ]
        }
      ],
      "source": [
        "business_df = spark.read.schema(schema).json('/content/drive/Shared with me/ISyE 6740 - Computational Analytics/Project/yelp_academic_dataset_business.json')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = spark.read.json('/content/drive/My Drive/1rBUldylKIYJm9jll5OSUCbz3duoT4Jfb/yelp_academic_dataset_review.json')"
      ],
      "metadata": {
        "id": "PEX39tczzbMC"
      },
      "id": "PEX39tczzbMC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = spark.read.json('/content/drive/My Drive/ISyE 6740 - Computational Analytics/Project/yelp_academic_dataset_user.json')"
      ],
      "metadata": {
        "id": "dAWiAAX1z5ne"
      },
      "id": "dAWiAAX1z5ne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split out attributes into own fields\n",
        "all_keys = business_df.selectExpr(\"explode(map_keys(attributes)) as keys\").select(\"keys\").distinct().collect()\n",
        "select_expr = [\"attributes.\" + key[\"keys\"] + \" as \" + key[\"keys\"] for key in all_keys]\n",
        "business_df = business_df.selectExpr(\"*\", *select_expr)\n",
        "business_df = business_df.drop(\"attributes\")"
      ],
      "metadata": {
        "id": "450PGOmoy6dD"
      },
      "id": "450PGOmoy6dD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to parse JSON strings\n",
        "def parse_json(json_str):\n",
        "    try:\n",
        "        parsed_json = eval(json_str)\n",
        "        return parsed_json if isinstance(parsed_json, dict) else {}\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "parse_json_udf = func.udf(parse_json, MapType(StringType(), BooleanType()))\n",
        "\n",
        "parseColumns = [\"Ambience\",\"BusinessParking\",\"BestNights\",\"GoodForMeal\",\"Music\"]\n",
        "\n",
        "for i in parseColumns:\n",
        "  business_df = business_df.withColumn(i, parse_json_udf(func.col(i)))\n",
        "\n",
        "  keys = business_df.selectExpr(f\"explode(map_keys({i})) as keys\").select(\"keys\").distinct().collect()\n",
        "  select_expr = [f\"{i}.\" + key[\"keys\"] + f\" as {i}_\" + key[\"keys\"] for key in keys]\n",
        "  business_df = business_df.selectExpr(\"*\", *select_expr)\n",
        "  business_df = business_df.drop(i)\n",
        "\n",
        "business_df.show()"
      ],
      "metadata": {
        "id": "rGda1ZQ4tYLu"
      },
      "id": "rGda1ZQ4tYLu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split for hours\n",
        "hour_keys = business_df.selectExpr(\"explode(map_keys(hours)) as keys\").select(\"keys\").distinct().collect()\n",
        "select_expr = [\"hours.\" + key[\"keys\"] + \" as hours_\" + key[\"keys\"] for key in hour_keys]\n",
        "business_df = business_df.selectExpr(\"*\", *select_expr)\n",
        "business_df = business_df.drop(\"hours\")"
      ],
      "metadata": {
        "id": "SwuOzqdG2RQc"
      },
      "id": "SwuOzqdG2RQc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26301c01",
      "metadata": {
        "id": "26301c01"
      },
      "outputs": [],
      "source": [
        "#Generic numeric user_id and business_id to use later for modeling functions\n",
        "user_window = Window.orderBy(\"user_id\")\n",
        "business_window = Window.orderBy(\"business_id\")\n",
        "\n",
        "users_df = users_df.withColumn(\"user_number\", func.row_number().over(user_window)) #May need to do this only for PA\n",
        "business_df = business_df.withColumn(\"business_number\", func.row_number().over(business_window))\n",
        "\n",
        "#Create temp views for SQL\n",
        "business_df.createOrReplaceTempView(\"business\")\n",
        "reviews_df.createOrReplaceTempView(\"reviews\")\n",
        "users_df.createOrReplaceTempView(\"users\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80748b1b",
      "metadata": {
        "id": "80748b1b"
      },
      "outputs": [],
      "source": [
        "#Data Filtering\n",
        "city = 'philadelphia'\n",
        "\n",
        "#Restaurants\n",
        "restaurant_sql = f'''\n",
        "        SELECT\n",
        "            b.*\n",
        "        FROM\n",
        "            business b\n",
        "        WHERE\n",
        "            b.is_open=1 AND LOWER(b.categories) LIKE '%restaurant%' AND lower(b.city) = '{city}'\n",
        "        ;\n",
        "    '''\n",
        "\n",
        "restaurants = spark.sql(restaurant_sql)\n",
        "restaurants.createOrReplaceTempView(\"restaurants\")\n",
        "\n",
        "\n",
        "#Reviews for Restaurants\n",
        "restaurant_reviews_sql = f'''\n",
        "        SELECT\n",
        "            rv.*, r.city, r.state\n",
        "        FROM\n",
        "            reviews rv\n",
        "        INNER JOIN\n",
        "            restaurants r\n",
        "        ON rv.business_id = r.business_id\n",
        "        ;\n",
        "    '''\n",
        "\n",
        "restaurant_reviews = spark.sql(restaurant_reviews_sql)\n",
        "restaurant_reviews.createOrReplaceTempView(\"restaurant_reviews\")\n",
        "\n",
        "#Restaurant review volume\n",
        "review_threshold=10\n",
        "restaurant_review_volume_sql = f'''\n",
        "        SELECT\n",
        "           rv.business_id, count(*) as review_count\n",
        "        FROM\n",
        "            restaurant_reviews rv\n",
        "        GROUP BY rv.business_id\n",
        "        HAVING count(*)>={review_threshold}\n",
        "        ;\n",
        "    '''\n",
        "restaurant_review_volume = spark.sql(restaurant_review_volume_sql)\n",
        "restaurant_reviews.createOrReplaceTempView(\"restaurant_review_volume\")\n",
        "\n",
        "#Restaurant review volume by state\n",
        "restaurant_review_volume_state_sql = f'''\n",
        "        SELECT\n",
        "           rv.state, count(*) as review_count\n",
        "        FROM\n",
        "            restaurant_reviews rv\n",
        "        GROUP BY rv.state ORDER BY review_count DESC\n",
        "        ;\n",
        "    '''\n",
        "\n",
        "restaurant_review_volume_state = spark.sql(restaurant_review_volume_state_sql)\n",
        "\n",
        "\n",
        "#Users with at least 10 restaurant reviews\n",
        "min_review_per_user = 10\n",
        "\n",
        "multi_restaurant_users_sql = f'''\n",
        "        SELECT\n",
        "            u.user_id, COUNT(DISTINCT r.business_id) AS restaurant_review_count\n",
        "        FROM\n",
        "            users u\n",
        "        INNER JOIN\n",
        "            restaurant_reviews r\n",
        "        ON u.user_id = r.user_id\n",
        "        INNER JOIN\n",
        "           restaurant_review_volume rrv\n",
        "        ON r.business_id = rrv.business_id\n",
        "        GROUP BY\n",
        "            u.user_id\n",
        "        HAVING\n",
        "            COUNT(DISTINCT r.business_id) >= {min_review_per_user}\n",
        "        ;\n",
        "    '''\n",
        "\n",
        "multi_restaurant_users = spark.sql(multi_restaurant_users_sql)\n",
        "multi_restaurant_users.createOrReplaceTempView(\"multi_restaurant_users\")\n",
        "\n",
        "\n",
        "#Data for restaurant reviews for users with at least 2 reviews\n",
        "final_reviews_sql = f'''\n",
        "        SELECT\n",
        "            r.user_id, ub.user_number, ub.average_stars, ub.elite, ub.fans, ub.friends, ub.name AS user_name, ub.review_count AS user_review_count, ub.yelping_since,\n",
        "            b.*,\n",
        "            r.review_id, r.date, r.stars AS review_stars, r.cool, r.funny, r.useful\n",
        "        FROM\n",
        "            restaurant_reviews r\n",
        "        INNER JOIN\n",
        "            multi_restaurant_users u\n",
        "        ON r.user_id = u.user_id\n",
        "        INNER JOIN\n",
        "            users ub\n",
        "        ON r.user_id = ub.user_id\n",
        "        INNER JOIN\n",
        "            restaurants b\n",
        "        ON r.business_id = b.business_id\n",
        "        ;\n",
        "    '''\n",
        "\n",
        "final_reviews = spark.sql(final_reviews_sql)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set some default values for certain fields\n",
        "final_reviews = final_reviews.fillna(False, subset=[\"Open24Hours\",\"DriveThru\",\"GoodForDancing\"])\n",
        "final_reviews = final_reviews.fillna(\"no\", subset=[\"Smoking\"])\n",
        "restaurants = restaurants.fillna(False, subset=[\"Open24Hours\",\"DriveThru\",\"GoodForDancing\"])\n",
        "restaurants = restaurants.fillna(\"no\", subset=[\"Smoking\"])\n",
        "\n",
        "# Drop columns manually if not filled in sufficiently / appear irrelevant to prediction\n",
        "cols_to_drop = [\"is_open\",\"longitude\",\"latitude\",\"address\",\"BYOB\",\"AcceptsInsurance\",\"DietaryRestrictions\",\"HairSpecializesIn\",\n",
        "                   \"ByAppointmentOnly\",\"RestaurantsCounterService\",\"BYOBCorkage\",\"CoatCheck\",\"BusinessAcceptsBitcoin\",\"AgesAllowed\",\"cool\",\"funny\",\"useful\", \"state\"]\n",
        "\n",
        "final_reviews_drop = final_reviews.drop(*cols_to_drop)\n",
        "restaurants_drop = restaurants.drop(*cols_to_drop)"
      ],
      "metadata": {
        "id": "o2jp2CkyengQ"
      },
      "id": "o2jp2CkyengQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38703a83",
      "metadata": {
        "id": "38703a83"
      },
      "outputs": [],
      "source": [
        "#For each user, pull 70% for training and 30% for testing - do on user basis, so we can test user's recommendations\n",
        "# Define the percentage for training and testing data\n",
        "train_percentage = 0.7\n",
        "test_percentage = 1 - train_percentage\n",
        "seed = 42\n",
        "\n",
        "#Random shuffle\n",
        "shuffled_final = final_reviews_drop.orderBy(func.rand(42))\n",
        "\n",
        "# Define window specification partitioned by the user_id column and corresponding row number\n",
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"user_id\")\n",
        "final_reviews_with_row_number = shuffled_final.withColumn(\"row_number\", func.row_number().over(window_spec))\n",
        "\n",
        "# Calculate the total number of rows for each user\n",
        "user_counts = final_reviews_with_row_number.groupBy(\"user_id\").count()\n",
        "\n",
        "# Calculate the number of rows to include in the training set for each user\n",
        "user_test_counts = user_counts.withColumn(\"test_count\", (func.col(\"count\") * test_percentage).cast(\"int\"))\n",
        "\n",
        "#Join row number data with test count\n",
        "final_with_test_counts = final_reviews_with_row_number.join(user_test_counts, \"user_id\", \"inner\")\n",
        "\n",
        "# Create training and testing DataFrames for each user\n",
        "train_data = final_with_test_counts.filter(func.col(\"row_number\") <= (func.col(\"count\") - func.col(\"test_count\")))\n",
        "test_data = final_with_test_counts.filter(func.col(\"row_number\") > (func.col(\"count\") - func.col(\"test_count\")))\n",
        "\n",
        "# Drop the intermediate columns\n",
        "train_data = train_data.drop(\"row_number\", \"count\", \"test_count\")\n",
        "test_data = test_data.drop(\"row_number\", \"count\", \"test_count\")\n",
        "\n",
        "#test_data.count()\n",
        "#train_data.count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hit_rate(ground_truth_df, recommended_df, user_col, item_col):\n",
        "    # Join ground truth and recommended items on user ID\n",
        "    joined_df = ground_truth_df.join(recommended_df, on=user_col)\n",
        "\n",
        "    # Filter to count hits (correct recommendations) for each user\n",
        "    hits_df = joined_df.filter(func.col(\"recommended_\" + item_col) == func.col(item_col)) \\\n",
        "                       .select(user_col).distinct()\n",
        "\n",
        "    # Count the distinct users with at least one correct recommendation\n",
        "    users_with_hits = hits_df.select(func.countDistinct(user_col)).collect()[0][0]\n",
        "\n",
        "    # Count the total number of distinct users for whom recommendations were made\n",
        "    total_users = recommended_df.select(func.countDistinct(user_col)).collect()[0][0]\n",
        "\n",
        "    # Calculate total hit rate\n",
        "    total_hit_rate = users_with_hits / total_users\n",
        "\n",
        "    return total_hit_rate\n",
        "\n",
        "\n",
        "def get_recall(ground_truth_df, recommended_df, user_col, item_col):\n",
        "    # Join ground truth and recommended items on user ID\n",
        "    joined_df = ground_truth_df.join(recommended_df, on=user_col)\n",
        "\n",
        "    # Count the number of relevant recommended items and total relevant items for each user\n",
        "    relevant_recommended_items = joined_df.filter(func.col(\"recommended_\"+item_col) == func.col(item_col))\n",
        "    total_relevant_items = ground_truth_df.groupBy(user_col).count()\n",
        "\n",
        "    # Compute recall for each user\n",
        "    recall_df = relevant_recommended_items.groupBy(user_col).count().withColumnRenamed(\"count\", \"relevant_count\") \\\n",
        "        .join(total_relevant_items, on=user_col)\n",
        "    recall_df = recall_df.withColumn(\"recall\", func.col(\"relevant_count\") / func.col(\"count\"))\n",
        "\n",
        "    # Average recall across all users\n",
        "    average_recall = recall_df.selectExpr(\"avg(recall)\").collect()[0][0]\n",
        "\n",
        "    return average_recall"
      ],
      "metadata": {
        "id": "TLZisOMf-8Jv"
      },
      "id": "TLZisOMf-8Jv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collaborative Filtering"
      ],
      "metadata": {
        "id": "dJSklMJzhfDN"
      },
      "id": "dJSklMJzhfDN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "014a2727",
      "metadata": {
        "id": "014a2727"
      },
      "outputs": [],
      "source": [
        "## Use Daemon's\n",
        "# #\n",
        "# als_features = ['user_number','business_number','review_stars']\n",
        "# cf_train_data = train_data.select(als_features)\n",
        "# cf_test_data = test_data.select(als_features)\n",
        "\n",
        "# #Build model\n",
        "# als = ALS(userCol=\"user_number\", regParam=0.1, itemCol=\"business_number\", ratingCol=\"review_stars\",\n",
        "#           coldStartStrategy=\"drop\")\n",
        "\n",
        "# #Tune parameters; limited due to memory\n",
        "# # paramGrid = ParamGridBuilder() \\\n",
        "# #     .addGrid(als.regParam, [0.01, 0.1, 1.0]) \\\n",
        "# #     .build()\n",
        "\n",
        "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"review_stars\", predictionCol=\"prediction\")\n",
        "\n",
        "# model = als.fit(cf_train_data)\n",
        "\n",
        "# # crossval = CrossValidator(estimator=als,\n",
        "# #                           estimatorParamMaps=paramGrid,\n",
        "# #                           evaluator=evaluator,\n",
        "# #                           numFolds=5)\n",
        "\n",
        "# # cvModel = crossval.fit(cf_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = cvModel.bestModel\n",
        "# best_rank = best_model.rank\n",
        "# best_max_iter = best_model._java_obj.parent().getMaxIter()\n",
        "# best_reg_param = best_model._java_obj.parent().getRegParam()"
      ],
      "metadata": {
        "id": "pFd9oI3eQQA-"
      },
      "id": "pFd9oI3eQQA-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0900d360",
      "metadata": {
        "id": "0900d360"
      },
      "outputs": [],
      "source": [
        "# #Evaluate the best model\n",
        "# predictions = model.transform(cf_test_data)\n",
        "# rmse = evaluator.evaluate(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# als_threshold_rating = 3.5\n",
        "# filtered_predictions = predictions.filter(predictions[\"prediction\"] >= als_threshold_rating).withColumnRenamed(\"business_number\", \"recommended_business_number\")\n",
        "\n",
        "# recall = get_recall(cf_test_data, filtered_predictions, \"user_number\", \"business_number\")\n",
        "# hit_rate = get_hit_rate(cf_test_data, filtered_predictions, \"user_number\", \"business_number\")\n",
        "# print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "# print(\"Avg Recall:\", recall)\n",
        "# print(\"Hit Rate:\", hit_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fco7DfF2HbLu",
        "outputId": "61aa2fe2-b4da-46d3-b2d0-0490f0df5306"
      },
      "id": "Fco7DfF2HbLu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 1.1654594490668244\n",
            "Avg Recall: 0.7148329583417334\n",
            "Total Recall: 0.689601250977326\n",
            "Hit Rate: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content-Based"
      ],
      "metadata": {
        "id": "tppPZ0Ohhjkm"
      },
      "id": "tppPZ0Ohhjkm"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#Pure restaurant information\n",
        "#restaurants\n",
        "\n",
        "# Filter only for 4+ reviews in training data - used at end, but not for cosine similarity calculation and remove user information except ID\n",
        "# Won't need user information, because similarity is based on business atributes\n",
        "review_4 = train_data.filter(func.col(\"review_stars\") >= 4)\n",
        "remove_user_attr = [\"fans\",'friends','elite',\"average_stars\",\"user_number\",\"user_name\",\"user_review_count\",\"yelping_since\"]\n",
        "review_4 = review_4.drop(*remove_user_attr)"
      ],
      "metadata": {
        "id": "_hm9i02XhlD3"
      },
      "id": "_hm9i02XhlD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input columns for each type\n",
        "string_columns = ['categories', 'city', 'WiFi', 'RestaurantsAttire', 'Alcohol', 'NoiseLevel']\n",
        "numeric_columns = ['stars','review_count','RestaurantsPriceRange2']\n",
        "boolean_columns = ['Ambience_romantic',\n",
        "    'Ambience_casual',\n",
        "    'Ambience_trendy',\n",
        "    'Ambience_intimate',\n",
        "    'Ambience_hipster',\n",
        "    'Ambience_upscale',\n",
        "    'Ambience_divey',\n",
        "    'Ambience_touristy',\n",
        "    'Ambience_classy',\n",
        "    'BusinessParking_valet',\n",
        "    'BusinessParking_lot',\n",
        "    'BusinessParking_validated',\n",
        "    'BusinessParking_garage',\n",
        "    'BusinessParking_street',\n",
        "    'BestNights_sunday',\n",
        "    'BestNights_thursday',\n",
        "    'BestNights_monday',\n",
        "    'BestNights_wednesday',\n",
        "    'BestNights_saturday',\n",
        "    'BestNights_friday',\n",
        "    'BestNights_tuesday',\n",
        "    'GoodForMeal_lunch',\n",
        "    'GoodForMeal_brunch',\n",
        "    'GoodForMeal_dinner',\n",
        "    'GoodForMeal_latenight',\n",
        "    'GoodForMeal_dessert',\n",
        "    'GoodForMeal_breakfast',\n",
        "    'Music_no_music',\n",
        "    'Music_dj',\n",
        "    'Music_live',\n",
        "    'Music_karaoke',\n",
        "    'Music_video',\n",
        "    'Music_background_music',\n",
        "    'Music_jukebox',\n",
        "    'GoodForDancing','RestaurantsReservations','OutdoorSeating','HasTV','RestaurantsTakeOut','BusinessAcceptsCreditCards','Open24Hours','HappyHour','DriveThru',\n",
        "    'WheelchairAccessible','Corkage','Caters','DogsAllowed','RestaurantsGoodForGroups','RestaurantsTableService','BikeParking','RestaurantsDelivery','GoodForKids']\n",
        "boolean_df = review_4.select(\"*\")\n",
        "\n",
        "\n",
        "#Convert field types to appropriate type\n",
        "to_num = [\"RestaurantsPriceRange2\"]\n",
        "to_bool = [\"GoodForDancing\",\"RestaurantsReservations\",\"OutdoorSeating\",\"HasTV\",\"RestaurantsTakeOut\",\"BusinessAcceptsCreditCards\",\"Open24Hours\",\"HappyHour\",\n",
        "           \"DriveThru\",\"WheelchairAccessible\",\"Corkage\",\"Caters\",\"DogsAllowed\",\"RestaurantsGoodForGroups\",\"RestaurantsTableService\",\"BikeParking\",\"RestaurantsDelivery\",\"GoodForKids\"]\n",
        "\n",
        "#Convert to int\n",
        "for c in to_num:\n",
        "    restaurants_drop = restaurants_drop.withColumn(c, func.col(c).cast(\"int\"))\n",
        "\n",
        "# Convert to boolean\n",
        "for c in to_bool:\n",
        "    restaurants_drop = restaurants_drop.withColumn(c, func.when(func.col(c) == \"True\", True).otherwise(False))\n",
        "\n",
        "\n",
        "# Remove fields with low variance\n",
        "for c in boolean_columns:\n",
        "  boolean_df = boolean_df.withColumn(c,func.col(c).cast('double'))\n",
        "\n",
        "boolean_df = boolean_df.select(boolean_columns)\n",
        "\n",
        "bool_variances = boolean_df.select([func.variance(func.col(c)).alias(c) for c in boolean_df.columns])\n",
        "bool_variances.show()\n",
        "\n",
        "remove_low_var = [\"DriveThru\",'Open24Hours',\"Music_video\",\"Music_no_music\",\"Music_karaoke\",\"GoodForDancing\"]\n",
        "review_4 = review_4.drop(*remove_low_var)\n",
        "restaurants_drop = restaurants_drop.drop(*remove_low_var)\n",
        "\n",
        "#Keeping some lower variance fields due to their ability to set restaurants apart - niche features ussers may be attracted to for a restaurant\n",
        "#There are features above like karaoke or GoodForDancing that may be relevant for lounges or other types of businesses, but excluding, because we're focused more on dining / restaurants"
      ],
      "metadata": {
        "id": "HykVVyMQy74u"
      },
      "id": "HykVVyMQy74u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Update relevant boolean column list\n",
        "\n",
        "boolean_columns = ['Ambience_romantic',\n",
        "    'Ambience_casual',\n",
        "    'Ambience_trendy',\n",
        "    'Ambience_intimate',\n",
        "    'Ambience_hipster',\n",
        "    'Ambience_upscale',\n",
        "    'Ambience_divey',\n",
        "    'Ambience_touristy',\n",
        "    'Ambience_classy',\n",
        "    'BusinessParking_valet',\n",
        "    'BusinessParking_lot',\n",
        "    'BusinessParking_validated',\n",
        "    'BusinessParking_garage',\n",
        "    'BusinessParking_street',\n",
        "    'BestNights_sunday',\n",
        "    'BestNights_thursday',\n",
        "    'BestNights_monday',\n",
        "    'BestNights_wednesday',\n",
        "    'BestNights_saturday',\n",
        "    'BestNights_friday',\n",
        "    'BestNights_tuesday',\n",
        "    'GoodForMeal_lunch',\n",
        "    'GoodForMeal_brunch',\n",
        "    'GoodForMeal_dinner',\n",
        "    'GoodForMeal_latenight',\n",
        "    'GoodForMeal_dessert',\n",
        "    'GoodForMeal_breakfast',\n",
        "    'Music_dj',\n",
        "    'Music_live',\n",
        "    'Music_background_music',\n",
        "    'Music_jukebox',\n",
        "    'RestaurantsReservations','OutdoorSeating','HasTV','RestaurantsTakeOut','BusinessAcceptsCreditCards','HappyHour',\n",
        "    'WheelchairAccessible','Corkage','Caters','DogsAllowed','RestaurantsGoodForGroups','RestaurantsTableService','BikeParking','RestaurantsDelivery','GoodForKids']"
      ],
      "metadata": {
        "id": "XeFdq0XXTU8J"
      },
      "id": "XeFdq0XXTU8J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# String feature encoding\n",
        "string_indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in string_columns]\n",
        "\n",
        "# One-hot encoding for string features\n",
        "encoder = OneHotEncoder(inputCols=[col+\"_indexed\" for col in string_columns],\n",
        "                        outputCols=[col+\"_encoded\" for col in string_columns], handleInvalid=\"keep\")\n",
        "\n",
        "# VectorAssembler for numeric columns\n",
        "numeric_assembler = VectorAssembler(inputCols=numeric_columns,\n",
        "                                    outputCol=\"numeric_features\", handleInvalid=\"keep\")\n",
        "\n",
        "# Normalizer for numeric features\n",
        "normalizer = Normalizer(inputCol=\"numeric_features\", outputCol=\"normalized_numeric_features\")\n",
        "\n",
        "# VectorAssembler for all features\n",
        "assembler = VectorAssembler(inputCols=[\"normalized_numeric_features\"] + [col for col in boolean_columns] + [col+\"_encoded\" for col in string_columns],\n",
        "                            outputCol=\"features\", handleInvalid=\"keep\")\n",
        "\n",
        "# Define pipeline\n",
        "stages = string_indexers + [encoder, numeric_assembler, normalizer, assembler]\n",
        "pipeline = Pipeline(stages=stages)"
      ],
      "metadata": {
        "id": "HdPZcuKF-qWp"
      },
      "id": "HdPZcuKF-qWp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the boolean columns and apply the conversion logic\n",
        "for col in boolean_columns:\n",
        "    restaurants_drop = restaurants_drop.withColumn(col, func.when(restaurants_drop[col], 1).otherwise(0))\n",
        "\n",
        "restaurants_drop = restaurants_drop.fillna(0, subset=numeric_columns)\n",
        "restaurants_drop = restaurants_drop.fillna(0, subset=boolean_columns)"
      ],
      "metadata": {
        "id": "_V68qVNnSNlr"
      },
      "id": "_V68qVNnSNlr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit and transform the pipeline\n",
        "#sample_df = restaurants_drop.sample(withReplacement=False,fraction=1000/restaurants_drop.count())\n",
        "model = pipeline.fit(restaurants_drop)\n",
        "transformed_df = model.transform(restaurants_drop)\n",
        "# Define a UDF to fill null values in a vector\n",
        "# def fill_null_with_zero(vector):\n",
        "#     return Vectors.dense([0.0 if x is None else x for x in vector.toArray()])\n",
        "\n",
        "# fill_null_with_zero_udf = udf(fill_null_with_zero, VectorUDT())\n",
        "# transformed_df = transformed_df.withColumn(\"features\", fill_null_with_zero_udf(\"features\"))\n",
        "\n",
        "transformed_df.show(truncate=False)\n",
        "\n",
        "# Compute Cartesian product to get all pairs of businesses\n",
        "cartesian_df = transformed_df.crossJoin(transformed_df.withColumnRenamed(\"business_id\", \"business_id2\").withColumnRenamed(\"features\", \"features2\"))"
      ],
      "metadata": {
        "id": "ERvSII2TB9I2"
      },
      "id": "ERvSII2TB9I2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute dot product of normalized feature vectors\n",
        "dot_udf = func.udf(lambda x, y: float(x.dot(y)), \"double\")\n",
        "dot_product_df = cartesian_df.withColumn(\"dot_product\", dot_udf(\"features\", \"features2\"))\n",
        "\n",
        "# Compute magnitudes of feature vectors\n",
        "magnitude_udf = func.udf(lambda x: float(x.norm(2)), \"double\")\n",
        "magnitude_df = dot_product_df.withColumn(\"magnitude\", magnitude_udf(\"features\")) \\\n",
        "    .withColumn(\"magnitude2\", magnitude_udf(\"features2\"))\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_similarity_df = magnitude_df.withColumn(\"cosine_similarity\",\n",
        "                                               func.col(\"dot_product\") / (func.col(\"magnitude\") * func.col(\"magnitude2\")))\n",
        "\n",
        "# Rank items by cosine similarity within each group (item)\n",
        "window = Window.partitionBy(\"business_id\").orderBy(func.col(\"cosine_similarity\").desc())"
      ],
      "metadata": {
        "id": "oHdK2vj20RYV"
      },
      "id": "oHdK2vj20RYV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top N similar items\n",
        "similarity_threshold = 0.7\n",
        "top_similar_items = cosine_similarity_df \\\n",
        "    .filter(func.col(\"business_id\") != func.col(\"business_id2\")) \\\n",
        "    .select(\"business_id\", \"business_id2\", \"cosine_similarity\") \\\n",
        "    .where(func.col(\"cosine_similarity\") >= similarity_threshold)"
      ],
      "metadata": {
        "id": "3KvWaDoSsjK3"
      },
      "id": "3KvWaDoSsjK3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the recommendations\n",
        "top_similar_items.show()"
      ],
      "metadata": {
        "id": "RysDypD6otkD"
      },
      "id": "RysDypD6otkD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_similar_items.count()"
      ],
      "metadata": {
        "id": "UzkopZ65wgin"
      },
      "id": "UzkopZ65wgin",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define ground truth and recommended df\n",
        "ground_truth_df = test_data.select(\"user_id\", \"business_id\")\n",
        "recommended_df = review_4.select(\"user_id\", \"business_id\").join(top_similar_items, on=\"business_id\", how=\"left\").select(\"user_id\",'business_id2').withColumnRenamed(\"business_id2\", \"recommended_business_id\").dropDuplicates()\n",
        "# recommended_df.show()\n",
        "# Calculate recall\n",
        "recall = get_recall(ground_truth_df, recommended_df, \"user_id\", \"business_id\")\n",
        "#total_recall = get_total_recall(ground_truth_df, recommended_df, \"user_id\", \"business_id\")\n",
        "hit_rate = get_hit_rate(ground_truth_df, recommended_df, \"user_id\", \"business_id\")\n",
        "print(\"Avg Recall:\", recall)\n",
        "#print(\"Total Recall:\", total_recall)\n",
        "print(\"Hit Rate:\", hit_rate)\n",
        "\n",
        "#Note, right now we're filtering for >=4 to use to make predictions - e.g., find restaurants similar to the ones that someone made a >=4 rating for,\n",
        "# but our test / ground truth right now includes everything (include lower reviews), so if we're only recommending \"good places\", do we also need to just filter upfront for positive reviews before we do test/train split"
      ],
      "metadata": {
        "id": "Dd0gKnnItTO2"
      },
      "id": "Dd0gKnnItTO2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "5d163a15",
      "metadata": {
        "id": "5d163a15"
      },
      "source": [
        "#Do we want to do the content-based? If we're able to process review text, could maybe do key word trends / sentiment analysis as part of it\n",
        "\n",
        "Not sure if we have enough information on the business to make informed decision - need to unnest some of the data because all my fields are captured under attributes. If we have enough attributes, can do content-based and knn to find \"similar\" restaurants\n",
        "\n",
        "Have issues with memory on my computer for the model fitting... Didn't try to do CV because it's already struggling\n",
        "\n",
        "If we use attributes, use PCA to cut down dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b7e044",
      "metadata": {
        "id": "72b7e044"
      },
      "source": [
        "Try to run in Google Colab\n",
        "\n",
        "Content-based with text analysis of reviews (TFIDF)\n",
        "\n",
        "Content-based with attributes unnested\n",
        "\n",
        "Schedule\n",
        "\n",
        "Step 1: Dataset Overview\n",
        "The Yelp dataset includes several types of information, but for our content-based system, we'll focus on:\n",
        "Businesses: Information about individual businesses, including their categories (e.g., restaurants, bars).\n",
        "Reviews: Textual reviews written by users for businesses.\n",
        "Step 2: Data Preprocessing\n",
        "Filter Restaurants: From the businesses data, filter out to keep only those in the 'Restaurants' category.\n",
        "Aggregate Reviews: For each restaurant, aggregate all its reviews into a single text document. This creates a comprehensive text representation of each restaurant's reviews.\n",
        "Text Preprocessing: Clean and preprocess the aggregated review texts for each restaurant. This involves steps like converting to lowercase, removing punctuation and stop words, and possibly stemming or lemmatization.\n",
        "Step 3: Feature Extraction\n",
        "TF-IDF: Apply Term Frequency-Inverse Document Frequency (TF-IDF) vectorization to the preprocessed review texts. This converts the textual data into a numerical format that captures the importance of words in relation to the document they appear in and the entire corpus of review texts. The result is a sparse matrix where each row represents a restaurant and each column represents a term's TF-IDF score.\n",
        "Step 4: User Profile Creation\n",
        "Identify User Preferences: For the target user, identify a set of restaurants they have rated highly or reviewed positively.\n",
        "Aggregate Preferred Reviews: Aggregate the review texts of these preferred restaurants to create a \"preference profile\" for the user. This text represents the kind of language and themes the user appears to favor in their preferred dining experiences.\n",
        "Vectorize User Profile: Apply the same TF-IDF vectorization used on the restaurant reviews to this aggregated preference profile text. This results in a TF-IDF vector representing the user's preferences.\n",
        "Step 5: Recommendation Generation\n",
        "Calculate Similarity: Compute the similarity between the user's preference vector and each restaurant's TF-IDF vector. This can be done using cosine similarity, which measures the cosine of the angle between two vectors, providing a similarity score between 0 and 1.\n",
        "Rank Restaurants: Rank the restaurants based on their similarity scores relative to the user's preference vector. Higher scores indicate a closer match to the user's preferences as inferred from their favored reviews.\n",
        "Step 6: Provide Recommendations\n",
        "Select Top-N: Select the top N restaurants with the highest similarity scores. These are the system's recommendations to the user, as they are the most similar to the user's expressed preferences based on past reviews.\n",
        "Example Output\n",
        "Assume our target user has shown a preference for restaurants with words like \"cozy\", \"artisanal coffee\", and \"homemade pastries\" frequently appearing in their positive reviews. The system might recommend restaurants whose aggregated reviews also frequently mention these terms, indicating a potential match in ambiance and offerings.\n",
        "Considerations\n",
        "Performance: Depending on the size of the dataset, steps like TF-IDF vectorization and similarity calculation can be computationally expensive. Techniques like dimensionality reduction or efficient similarity search algorithms can help mitigate this.\n",
        "Cold Start Problem: New users or restaurants without sufficient reviews pose a challenge, as there's little data to base recommendations on. Hybrid approaches or content-based features not reliant on user interaction can help address this.\n",
        "Diversity and Serendipity: Pure content-based systems might over-specialize in recommending items too similar to what the user has already experienced. Incorporating elements to introduce diversity and serendipity can enhance user satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "689fa0d9",
      "metadata": {
        "id": "689fa0d9"
      },
      "source": [
        "Performance - Use recall for content-based AND collaborative filtering; we also have RMSE\n",
        "\n",
        "\n",
        "Content-based filtering is a recommendation technique that utilizes information about the items themselves to make recommendations. When evaluating the performance of a content-based filtering system, you can use various metrics to assess its effectiveness. Here are some common performance metrics for content-based filtering:\n",
        "\n",
        "Precision and Recall: Precision measures the proportion of recommended items that are relevant to the user, while recall measures the proportion of relevant items that are recommended to the user. These metrics are typically calculated at different levels, such as precision@k and recall@k, where k is the number of recommended items.\n",
        "\n",
        "F1 Score: The F1 score is the harmonic mean of precision and recall and provides a single metric that balances both precision and recall. It's calculated as\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        " .\n",
        "\n",
        "Mean Average Precision (MAP): MAP measures the average precision across all users. It considers both the relevance of recommended items and their rank in the list of recommendations.\n",
        "\n",
        "Normalized Discounted Cumulative Gain (NDCG): NDCG evaluates the ranking quality of recommended items by considering both relevance and rank. It penalizes items that are ranked lower in the list of recommendations.\n",
        "\n",
        "Mean Reciprocal Rank (MRR): MRR measures the average rank of the first relevant item in the list of recommendations. It provides an indication of how quickly relevant items are found.\n",
        "\n",
        "Coverage: Coverage measures the proportion of items in the catalog that are recommended to at least one user. It provides insights into the diversity of recommendations.\n",
        "\n",
        "Novelty: Novelty measures the degree to which recommended items are different from those that the user has interacted with before. It helps ensure that recommendations introduce users to new and interesting items.\n",
        "\n",
        "User Satisfaction: User satisfaction can be assessed through user feedback, surveys, or other qualitative measures. It provides insights into whether users find the recommendations useful and relevant.\n",
        "\n",
        "When evaluating a content-based filtering system, it's essential to consider a combination of these metrics to gain a comprehensive understanding of its performance. Additionally, the choice of metrics may depend on the specific goals and characteristics of the recommendation system and the domain in which it operates."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c838d8f9",
      "metadata": {
        "id": "c838d8f9"
      },
      "source": [
        "Schedule\n",
        "Haley to commit data to GitHub for Daemon\n",
        "\n",
        "Due: Apr 27\n",
        "\n",
        "Try converting to parquet file first, but it might impact nested json\n",
        "\n",
        "Tues @ 9 PM EST - sync; have models ready with recall calculation for each model\n",
        "\n",
        "Paper (can start Wed and finish initial draft by Fri and final edits to submit Sat); do we want to meet Fri or Sat before submitting to review or just review / comment async?\n",
        "- Preprocessing\n",
        "  - Text analysis\n",
        "  - Train / test split\n",
        "- Analyze / compare results - does one model do better at recall for certain types of restaurants than the other; what about general performance (recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29c82f3",
      "metadata": {
        "id": "c29c82f3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}